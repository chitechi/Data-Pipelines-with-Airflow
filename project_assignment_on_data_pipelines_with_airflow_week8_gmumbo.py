# -*- coding: utf-8 -*-
"""Project Assignment on Data Pipelines with Airflow Week8_GMumbo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Cfg9RI5x2dur2lTwJNi5SLtCxkoF3Igx
"""

!pip install apache-airflow

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import pandas as pd
import psycopg2
import logging

# DAG default arguments
default_args = {
    'owner': 'MTN Rwanda Telecoms',
    'depends_on_past': False,
    'start_date': datetime(2023, 3, 19),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

# DAG Definition
dag = DAG('data_pipeline', 
          default_args=default_args, 
          schedule_interval=timedelta(days=1)
          )

# Data extraction from the CSV files
def extract_data():
    customer_df = pd.read_csv('customer_data.csv')
    order_df = pd.read_csv('order_data.csv')
    payment_df = pd.read_csv('payment_data.csv')
    return customer_df, order_df, payment_df

# Transform the data
def transform_data(customer_df, order_df, payment_df):
    # date_of_birth field to datetime format conversion
    customer_df['date_of_birth'] = pd.to_datetime(customer_df['date_of_birth'])

    # customer and order dataframes on the customer_id column merging
    customer_order_df = pd.merge(customer_df, order_df, on='customer_id')

    # payment dataframe with the merged dataframe on the order_id and customer_id columns
    customer_payment_df = pd.merge(customer_order_df, payment_df, on=['order_id', 'customer_id'])

    # Drop unnecessary columns
    customer_payment_df.drop(columns=['customer_id', 'order_id'], inplace=True)

    # Group the data by customer and aggregate the amount paid
    customer_grouped_df = customer_payment_df.groupby(['first_name', 'last_name', 'email', 'country', 'gender', 'date_of_birth'])['amount'].sum().reset_index()

    # Calculate the total value of orders made by each customer in a new column
    customer_grouped_df['total_order_value'] = customer_payment_df.groupby(['first_name', 'last_name', 'email', 'country', 'gender', 'date_of_birth'])['price'].sum().values
    
    # Calculate the customer lifetime value using the formula CLV = (average order value) x (number of orders made per year) x (average customer lifespan)
    customer_grouped_df['average_order_value'] = customer_grouped_df['total_order_value'] / customer_grouped_df['amount']
    customer_grouped_df['number_of_orders_per_year'] = customer_grouped_df['amount'] / ((pd.to_datetime('now') - customer_grouped_df['date_of_birth']).dt.days / 365)
    customer_grouped_df['average_customer_lifespan'] = (pd.to_datetime('now') - customer_grouped_df['date_of_birth']).dt.days / 365
    customer_grouped_df['clv'] = customer_grouped_df['average_order_value'] * customer_grouped_df['number_of_orders_per_year'] * customer_grouped_df['average_customer_lifespan']
    return customer_grouped_df

# Load the transformed data into a PostgreSQL database
def load_data(transformed_df):
    try:
        # Connect to the PostgreSQL database
        conn = psycopg2.connect(
            host = "34.170.193.146"
            database = "RwandaMTNDB"
            user = "admin"
            password = "admin1"
        )

        # Open a cursor to perform database operations
        cur = conn.cursor()

        # Create the customer_ltv table
        cur.execute("""
            CREATE TABLE IF NOT EXISTS customer_ltv (
                customer_id INTEGER PRIMARY KEY,
                total_orders INTEGER,
                total_amount NUMERIC(10,2),
                avg_order_value NUMERIC(10,2),
                ltv NUMERIC(10,2)
            )
        """)

        # Insert the transformed data into the customer_ltv table
        for index, row in transformed_df.iterrows():
            cur.execute("""
                INSERT INTO customer_ltv (customer_id, total_orders, total_amount, avg_order_value, ltv)
                VALUES (%s, %s, %s, %s, %s)
            """, (row['customer_id'], row['total_orders'], row['total_amount'], row['avg_order_value'], row['ltv']))

        conn.commit()

        # Close the cursor and connection
        cur.close()
        conn.close()

        # success message
        logging.info("Data loaded successfully into PostgreSQL database")

    except Exception as e:
        # error message
        logging.error(f"Error loading data into PostgreSQL database: {str(e)}")
        raise e

# extract data task
extract_data_task = PythonOperator(
    task_id='extract_data',
    python_callable=extract_data,
    dag=dag
)

# transform data task
transform_data_task = PythonOperator(
    task_id='transform_data',
    python_callable=transform_data,
    dag=dag
)

# load data task
load_data_task = PythonOperator(
    task_id='load_data',
    python_callable=load_data,
    dag=dag
)

# task dependencies
extract_data_task >> transform_data_task >> load_data_task